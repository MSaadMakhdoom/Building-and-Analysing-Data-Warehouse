# Building-and-Analysing-Data-Warehouse
 course instructor :DR Muhammad Asif Naeem
 
2. Project overview
METRO is one of the biggest superstores chains in Pakistan. The stores has thousands of
customers and therefore it is important for the store to online analyse the shopping behaviour of
their customers. Based on that the store can optimise their selling techniques e.g. giving of
promotions on different products.


Now, to make this analysis of shopping behaviour practical there is a need of building a near-real-
time DW and customers’ transactions from Data Sources (DSs) are required to reflect into DW as

soon as they appear in DSs. The overview of METRO DW is presented in Figure 1. To build a near-
real-time DW we need to implement a near-real-time ETL (Extraction, Transformation, and

Loading) tools. Since the data generated by customers is not in the format required by DW
therefore, it needs to process in the transformation layer of ETL. For example enriching of some
information e.g. attributes in colour red from Master Data (MD) as shown in Figure 2.


To implement this enrichment feature in the transformation phase of ETL we need a join operator.
There are a number of algorithms available to implement this join operation however, the most
popular one is MESHJOIN (Mesh Join) which is explained in next section and you will implement it
in this project using Java SE 8 with Eclipse IDE.

3. MESHJOIN (Mesh Join)
The MESHJOIN (Mesh Join) algorithm has been introduced by Polyzotis in 2008 with objective of
implementing the join operation in the transformation phase of ETL.
The main components of MESHJOIN are: The disk-buffer which will be an array and used to load
the disk partitions in memory. Typically, MD is large, it has to be loaded in memory in partitions.
Normally, the size of each partition in MD is equal to the size of the disk-buffer. Also MD is
traversed cyclically in an endless loop. The hash table which stores the customers’ transactions
(tuples). The queue is used to keep the record of all the customers’ transactions in memory with
respect to their arrival times. The queue has same number of partitions as MD to make sure that
each tuple has joined with the whole MD before leaving the join operator. The stream-buffer will
be an array and is used to hold the customer transaction meanwhile the algorithm completes one
iteration. However, you don’t need the stream buffer in this project as we are not considering the
stream of customers’ transactions.


The crux of MESHJOIN is that with every loop step a new chunk of customers’ transactions is read
into main memory (Hash table) and MD partition in the disk-buffer is replaced by the new MD
partition from the disk. Each of these chunks will remain in main memory for the time of one full
MD cycle. The chunks therefore leave main memory in the order that they enter main memory
and their time of residence in main memory is overlapping. This leads to the staggered processing
pattern of MESHJOIN. In main memory, the incoming customers’ data is organized in a queue,
each chunk being one element of the queue. Figure 3 with four MD partitions shows a pictorial


representation of the MESHJOIN operation: at each point in time, each chunk Si in the queue has
seen a larger number of partitions than the previous, and started at a later position in MD (except
for the case that the traversal of MD resets to the start of MD). The figure shows the moment
when partition R2 of MD is read into the disk-buffer but is not yet processed.
After loading the disk partition into the disk buffer, the algorithm probes each tuple of the disk
buffer in the hash table. If a matching tuple is found, the algorithm generates the join output.
After each iteration the algorithm removes the oldest chunk of customers’ transactions from the
hash table along with their pointers from the queue. This chunk is found at the end of the queue;
its tuples were joined with the whole of MD and are thus completely processed now.
4. Star-schema
The star schema (which you will use in this project) is a data modelling technique that is used to
map multidimensional decision support data into a relational database. Star-schema yields an
easily implemented model for multidimensional data analysis while still preserving the relational
structures on which the operational database is built.
The star schema represents aggregated data for specific business activities. Using the schema, one
can create multiple aggregated data sources that will represent different aspects of business
operations. For example, the aggregation may involve total sales by selected time periods, by
products, by stores, and so on. Aggregated totals can be total product sold, total sales values by
products, etc. The basic star schema has three main components: facts, dimensions, attributes, and
classification levels. Usually in case of star-schema for sales the dimension tables are: product,
date, store, and supplier while the fact table is sales. However, to determine the right attributes


5. Data specifications
The assessment provides a MySQL scripts file named “Transaction_and_MasterData_Generator.sql”.
By executing the script using MySQL database it will create two tables in your account. One is
TRANSACTIONS table with 10,000 records populated in it. This data will be generated randomly

based on 100 products, 50 customers, 10 stores, and one year time period as a date - from 01-Jan-
16 to 31-Dec-16. The values for the quantity attribute will be random between 1 and 10. The other is

MASTERDATA table with 100 records in it. The structure of both tables with their attributes name
and data types is given below in Figure 4. The attributes TRANSACTION_ID and PRODUCT_ID are
primary keys in TRANSACTIONS and MASTERDATA tables respectively.


You will divide MD into 10 equal size partitions (10 tuples in each partition) and therefore, the
total number of partitions in the queue (in MESHJOIN) will also be 10 while the size of each
partition (in terms of tuples’ pointers) will be 50 as in each iteration the algorithm will load 50
tuples from transaction table to memory.


7. DW analysis
Once the entire data has been loaded into DW, you will be required to analyse your DW by
applying following OLAP queries.
Q1 Present total sales of all products supplied by each supplier with respect to quarter and
month
Q2 Present total sales of each product sold by each store. The output should be organised
store wise and then product wise under each store.
Q3 Find the 5 most popular products sold over the weekends.
Q4 Present the quarterly sales of each product for year 2016 using drill down query concept.
Note: each quarter sale must be a column.
Q5 Extract total sales of each product for the first and second half of year 2016 along with its
total yearly sales.
Q6 Find an anomaly in the data warehouse dataset. write a query to show the anomaly and
explain the anomaly in your project report.

Q7 Create a materialised view with name “STOREANALYSIS_MV” that presents the product-
wise sales analysis for each store.


